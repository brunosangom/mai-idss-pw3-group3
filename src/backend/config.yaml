
# Data configuration
data:
  path: "data/Wildfire_Dataset.csv"
  features:
    - "pr"
    - "rmax"
    - "rmin"
    - "sph"
    - "srad"
    - "tmmn"
    - "tmmx"
    - "vs"
    - "bi"
    - "fm100"
    - "fm1000"
    - "erc"
    - "etr"
    - "pet"
    - "vpd"
  split_ratios:
    train: 0.8
    val: 0.1
    test: 0.1
  window_size: 30
  # Temporal split strategy: controls how data is divided into train/val/test splits
  # Options:
  #   - 'sequential': Original chronological split (first X% -> train, next Y% -> val, rest -> test)
  #                   WARNING: May cause temporal distribution shift if wildfire rates vary over time
  #   - 'month': Assigns entire months to splits in round-robin fashion (balanced across time)
  #   - 'quarter': Assigns entire quarters to splits in round-robin fashion (recommended)
  #   - 'year': Assigns entire years to splits in round-robin fashion (coarsest granularity)
  # The stratified options (month/quarter/year) ensure each split contains data from different
  # time periods, preventing temporal distribution shift while maintaining temporal continuity
  # within each bucket for proper sequence creation.
  temporal_bucket: "quarter"

# Model configuration
model:
  name: "Transformer"
  params:
    d_model: 32
    nhead: 4
    num_layers: 2
    dim_feedforward: 512
    dropout: 0.1
  threshold: 0.01

# Training configuration
training:
  batch_size: 32
  epochs: 1
  max_steps_per_epoch: 1000
  learning_rate: 0.001
  loss_function: "BCE"
  optimizer: "Adam"
  tune_threshold: True  # If True, tunes threshold on validation set based on F1 score after training
  metrics:
    - "Precision"
    - "Recall"
    - "F1Score"
    - TrueWildfires
    - PredictedWildfires

# System configuration
system:
  seed: 42  # Random seed for reproducibility (set to null to disable)
  log_dir: "src/backend/logs/"
  results_dir: "src/backend/results/"
  checkpoint_path: "src/backend/checkpoints/best_model.pt"
  verbose: True
